name: PredictiveMaintenance Project Pipeline

on:
  push:
    branches:
      - main  # Automatically triggers on push to the main branch

jobs:

  register-dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Specify the Python version you used in Colab

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          # Install dependencies from your requirements file
          pip install -r PredictiveMaintenanceDataset/model_building/requirements.txt
          # Install additional dependencies needed for model building and data registration
          pip install huggingface_hub pandas scikit-learn xgboost

      - name: Upload Dataset to Hugging Face Hub
        env:
          MLOps_token: ${{ secrets.HF_TOKEN }} # Use the GitHub secret for the HF token
        run: |
          # Set the environment variable for the script
          export MLOps_token=${{ secrets.HF_TOKEN }}
          python PredictiveMaintenanceDataset/model_building/data_register.py


  data-prep:
    needs: register-dataset
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Specify the Python version

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r PredictiveMaintenanceDataset/model_building/requirements.txt
          pip install huggingface_hub pandas scikit-learn

      - name: Run Data Preparation
        env:
          MLOps_token: ${{ secrets.HF_TOKEN }} # Use the GitHub secret for the HF token
        run: |
          export MLOps_token=${{ secrets.HF_TOKEN }}
          python PredictiveMaintenanceDataset/model_building/prep.py

  model-traning:
    needs: data-prep
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Specify the Python version

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r PredictiveMaintenanceDataset/model_building/requirements.txt
          pip install huggingface_hub pandas scikit-learn xgboost mlflow==3.0.1

      - name: Start MLflow Server # Note: Running MLflow server like this in GitHub Actions is typically for tracking within the run, not for persistent access.
        run: |
          # This is a placeholder. For persistent MLflow tracking, you'd typically use a dedicated MLflow server.
          echo "MLflow server setup is not implemented in this example workflow."

      - name: Model Building
        env:
          MLOps_token: ${{ secrets.HF_TOKEN }} # Use the GitHub secret for the HF token
        run: |
          export MLOps_token=${{ secrets.HF_TOKEN }}
          python PredictiveMaintenanceDataset/model_building/train.py

  deploy-hosting:
    runs-on: ubuntu-latest
    needs: [model-traning,data-prep,register-dataset] # Ensure all previous steps are complete
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9' # Specify the Python version

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r PredictiveMaintenanceDataset/model_building/requirements.txt
          pip install huggingface_hub joblib

      - name: Push files to Frontend Hugging Face Space
        env:
          MLOps_token: ${{ secrets.HF_TOKEN }} # Use the GitHub secret for the HF token
          HF_SPACE_REPO_ID: sammysri12/PredictiveMaintenance-MLOps # Replace with your Space repo ID
        run: |
          export MLOps_token=${{ secrets.HF_TOKEN }}
          export HF_SPACE_REPO_ID=${{ env.HF_SPACE_REPO_ID }}
          python -c "
import os
from huggingface_hub import HfApi

api = HfApi(token=os.getenv('MLOps_token'))
local_folder = 'PredictiveMaintenanceDataset/model_building' # Corrected local folder path
space_repo_id = os.getenv('HF_SPACE_REPO_ID')

try:
    print(f'Uploading files from {local_folder} to Space: {space_repo_id}')
    api.upload_folder(
        folder_path=local_folder,
        repo_id=space_repo_id,
        repo_type='space',
        commit_message='Update deployment files from GitHub Actions'
    )
    print(f'Deployment files uploaded successfully to Hugging Face Space: {space_repo_id}')
except Exception as e:
    print(f'Error uploading deployment files: {e}')
    exit(1) # Exit with a non-zero status code to indicate failure
"
